dtype: fp16
bs: 1
seed:
  - 42
  - 42
  - 42
  - 42

pretrained_model: 'Meina/MeinaMix_V11'
prompt:
  - 'best quality, masterpiece, highres, solo, {virtuosa_arknights-${lora.step}:0.9}, long_hair, bangs, black_hair, halo, smile, closed_mouth, ascot, very_long_hair, wings, black_eyes, blunt_bangs, 1girl, collared_shirt, shirt, looking_at_viewer, mole, mole_under_eye, simple_background, white_background, white_shirt, black_ascot, portrait, upper_body'
  - 'best quality, masterpiece, highres, solo, {virtuosa_arknights-${lora.step}:0.9}, long_hair, bangs, black_hair, halo, smile, closed_mouth, ascot, very_long_hair, wings, black_eyes, blunt_bangs, 1girl, black_gloves, black_skirt, black_thighhighs, gloves, looking_at_viewer, shirt, skirt, thighhighs, white_shirt, black_footwear, collared_shirt, high_heels, holding, full_body, simple_background, elbow_gloves, grey_eyes, white_background'
  - 'best quality, masterpiece, highres, solo, {night:1.10}, {starry sky:1.10}, beach, beautiful detailed sky, {extremely detailed background:1.20}, {virtuosa_arknights-${lora.step}:0.9}, {standing:1.10}, looking at viewer, {bikini:1.30}, long_hair, bangs, black_hair, halo, smile, closed_mouth, ascot, very_long_hair, wings, black_eyes, blunt_bangs, light smile'
  - 'best quality, masterpiece, highres, solo, {virtuosa_arknights-${lora.step}:0.9}, long_hair, bangs, black_hair, halo, smile, closed_mouth, ascot, very_long_hair, wings, black_eyes, blunt_bangs'
neg_prompt:
  - '{worst quality, low quality:1.40}, {zombie, sketch, interlocked fingers, comic:1.10}, {full body:1.10}, lowres, bad anatomy, bad hands, text, error, missing fingers, extra digit, fewer digits, cropped, worst quality, low quality, normal quality, jpeg artifacts, signature, watermark, username, blurry, white border, {english text, chinese text:1.05}'
  - '{worst quality, low quality:1.40}, {zombie, sketch, interlocked fingers, comic:1.10}, {full body:1.10}, lowres, bad anatomy, bad hands, text, error, missing fingers, extra digit, fewer digits, cropped, worst quality, low quality, normal quality, jpeg artifacts, signature, watermark, username, blurry, white border, {english text, chinese text:1.05}'
  - '{worst quality, low quality:1.40}, {zombie, sketch, interlocked fingers, comic:1.10}, {full body:1.10}, lowres, bad anatomy, bad hands, text, error, missing fingers, extra digit, fewer digits, cropped, worst quality, low quality, normal quality, jpeg artifacts, signature, watermark, username, blurry, white border, {english text, chinese text:1.05}'
  - '{worst quality, low quality:1.40}, {zombie, sketch, interlocked fingers, comic:1.10}, {full body:1.10}, lowres, bad anatomy, bad hands, text, error, missing fingers, extra digit, fewer digits, cropped, worst quality, low quality, normal quality, jpeg artifacts, signature, watermark, username, blurry, white border, {english text, chinese text:1.05}'
N_repeats: 3

clip_skip: 1 #动漫模型通常会跳过一个CLIP层
models_dir: exps/virtuosa_t4_r16_w1_p0.3_initc_dunet0.01_note/ckpts
emb_dir: ${models_dir}

infer_args:
  init_width: 512
  init_height: 768
  width: 1024
  height: 1536  # image size
  guidance_scale: 7  # scale, when higher, the images will tend to be more similar
  num_inference_steps: 30  # how many steps
  scheduler: # DPM++ 2M Karras
    _target_: diffusers.DPMSolverMultistepScheduler
    beta_start: 0.00085
    beta_end: 0.012
    algorithm_type: dpmsolver++
    beta_schedule: scaled_linear
    use_karras_sigmas: true

output_dir: 'output/'
lora:
  alpha: 0.8
  dir: ${models_dir}
  step: 2000
  text_encoder: ${lora.dir}/text_encoder-${lora.step}.safetensors
  unet: ${lora.dir}/unet-${lora.step}.safetensors

memory: { }

prepare:
  - _target_: hcpdiff.workflow.LoadModelsAction
    pretrained_model: ${pretrained_model}
    dtype: ${dtype}
    scheduler: ${infer_args.scheduler}
    vae: # use NAI's vae
      _target_: diffusers.AutoencoderKL.from_pretrained
      pretrained_model_name_or_path: deepghs/animefull-latest  # path to vae model
      subfolder: vae
  - _target_: hcpdiff.workflow.BuildModelLoaderAction

  # load lora
  # - _target_: hcpdiff.workflow.LoadLoraAction
  #   model: 'TE'
  #   cfg:
  #     - path: ${lora.text_encoder}
  #       alpha: ${lora.alpha}
  - _target_: hcpdiff.workflow.LoadLoraAction
    model: 'unet'
    cfg:
      - path: ${lora.unet}
        alpha: ${lora.alpha}
  - _target_: hcpdiff.workflow.XformersEnableAction
  - _target_: hcpdiff.workflow.ExecAction
    prog: |-
      import torch
      from hcpdiff.utils.net_utils import to_cpu, to_cuda
      to_cuda(memory.unet)
      to_cuda(memory.text_encoder)
      memory.vae.to(dtype=torch.bfloat16)
      #to_cuda(memory.vae)
  - _target_: hcpdiff.workflow.PrepareDiffusionAction
    dtype: ${dtype}
  - _target_: hcpdiff.workflow.VaeOptimizeAction
    slicing: True

actions:
  - _target_: hcpdiff.workflow.TextHookAction # text encoder and tokenizer auto get from memory
    N_repeats: ${N_repeats}
    layer_skip: ${clip_skip}
    emb_dir: ${emb_dir}

  # encode text
  - _target_: hcpdiff.workflow.AttnMultTextEncodeAction
    prompt: ${prompt}
    negative_prompt: ${neg_prompt}
    bs: ${bs}
  # prepare seed
  - _target_: hcpdiff.workflow.SeedAction
    seed: ${seed}
  - _target_: hcpdiff.workflow.MakeTimestepsAction
    N_steps: ${infer_args.num_inference_steps}
  # text to image
  - _target_: hcpdiff.workflow.MakeLatentAction
    width: ${infer_args.init_width}
    height: ${infer_args.init_height}
  - _target_: hcpdiff.workflow.LoopAction
    loop_value:
      timesteps: t
    actions:
      - _target_: hcpdiff.workflow.DiffusionStepAction
        guidance_scale: ${infer_args.guidance_scale}

  # image to image
  - _target_: hcpdiff.workflow.LatentResizeAction
    width: ${infer_args.width}
    height: ${infer_args.height}
  - _target_: hcpdiff.workflow.SeedAction
    seed: ${seed}
  - _target_: hcpdiff.workflow.MakeTimestepsAction
    N_steps: ${infer_args.num_inference_steps}
  # only part of timesteps
  - _target_: hcpdiff.workflow.ExecAction
    prog: |-
      states['timesteps'] = states['timesteps'][int(${infer_args.num_inference_steps}*(1-0.6)):]
      states['start_timestep'] = states['timesteps'][:1]
  - _target_: hcpdiff.workflow.MakeLatentAction
    width: ${infer_args.width}
    height: ${infer_args.height}
  - _target_: hcpdiff.workflow.LoopAction
    loop_value:
      timesteps: t
    actions:
      - _target_: hcpdiff.workflow.DiffusionStepAction
        guidance_scale: ${infer_args.guidance_scale}

  # decode to image
  - _target_: hcpdiff.workflow.ExecAction
    prog: |-
      from hcpdiff.utils.net_utils import to_cpu, to_cuda
      to_cpu(memory.unet)
  - _target_: hcpdiff.workflow.DecodeAction
    vae: ${hcp.from_memory:vae}
    offload: true
  - _target_: hcpdiff.workflow.SaveImageAction
    save_root: ${output_dir}
    image_type: png
