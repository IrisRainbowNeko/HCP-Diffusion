dtype: fp16
bs: 1
seed: 42

pretrained_model: 'KBlueLeaf/kohaku-xl-beta7.1'
prompt: '1girl loli loli solo wings dragon-horns dragon-wings cup red-hair purple-eyes sitting crossed-legs flower teacup looking-at-viewer red-flower white-dress dress one-side-up small-breasts holding breasts barefoot table navel stomach underboob bare-shoulders holding-cup short-hair bangs teapot sleeveless-dress cloudy-sky river forest tree sunset expressionless smile sideboob'
neg_prompt: 'bad-anatomy watermarks text blurry'
N_repeats: 1

clip_skip: 1 #动漫模型通常会跳过一个CLIP层

infer_args:
  init_width: 576
  init_height: 960
  width: 1152
  height: 1920  # image size
  guidance_scale: 5  # scale, when higher, the images will tend to be more similar
  num_inference_steps: 20  # how many steps

output_dir: 'output/'

memory: { }

prepare:
  - _target_: hcpdiff.workflow.LoadModelsAction
    pretrained_model: ${pretrained_model}
    dtype: ${dtype}
    scheduler: # DPM++ 2M Karras
      _target_: diffusers.DPMSolverMultistepScheduler
      beta_start: 0.00085
      beta_end: 0.012
      algorithm_type: dpmsolver++
      beta_schedule: scaled_linear
      use_karras_sigmas: true
    vae: # use NAI's vae
      _target_: diffusers.AutoencoderKL.from_pretrained
      pretrained_model_name_or_path: stabilityai/stable-diffusion-xl-base-1.0  # path to vae model
      subfolder: vae
  - _target_: hcpdiff.workflow.XformersEnableAction
  - _target_: hcpdiff.workflow.ExecAction
    prog: |-
      import torch
      from hcpdiff.utils.net_utils import to_cpu, to_cuda
      to_cuda(memory.unet)
      to_cuda(memory.text_encoder)
      memory.vae.to(dtype=torch.bfloat16)
      #to_cuda(memory.vae)
  - _target_: hcpdiff.workflow.PrepareDiffusionAction
    dtype: ${dtype}
  - _target_: hcpdiff.workflow.VaeOptimizeAction
    slicing: True

actions:
  - _target_: hcpdiff.workflow.TextHookAction # text encoder and tokenizer auto get from memory
    N_repeats: ${N_repeats}
    layer_skip: ${clip_skip}
    TE_final_norm: false

  # encode text
  - _target_: hcpdiff.workflow.AttnMultTextEncodeAction
    prompt: ${prompt}
    negative_prompt: ${neg_prompt}
    bs: ${bs}
  # prepare seed
  - _target_: hcpdiff.workflow.SeedAction
    seed: ${seed}
  - _target_: hcpdiff.workflow.MakeTimestepsAction
    N_steps: ${infer_args.num_inference_steps}
  # text to image
  - _target_: hcpdiff.workflow.MakeLatentAction
    width: ${infer_args.init_width}
    height: ${infer_args.init_height}
  - _target_: hcpdiff.workflow.LoopAction
    loop_value:
      timesteps: t
    actions:
      - _target_: hcpdiff.workflow.DiffusionStepAction
        guidance_scale: ${infer_args.guidance_scale}

  # image to image
  - _target_: hcpdiff.workflow.LatentResizeAction
  - _target_: hcpdiff.workflow.SeedAction
    seed: ${seed}
  - _target_: hcpdiff.workflow.MakeTimestepsAction
    N_steps: ${infer_args.num_inference_steps}
  # only part of timesteps
  - _target_: hcpdiff.workflow.ExecAction
    prog: |-
      states['timesteps'] = states['timesteps'][int(30*(1-0.6)):]
      states['start_timestep'] = states['timesteps'][:1]
  - _target_: hcpdiff.workflow.MakeLatentAction
    width: ${infer_args.width}
    height: ${infer_args.height}
  - _target_: hcpdiff.workflow.LoopAction
    loop_value:
      timesteps: t
    actions:
      - _target_: hcpdiff.workflow.DiffusionStepAction
        guidance_scale: ${infer_args.guidance_scale}

  # decode to image
  - _target_: hcpdiff.workflow.ExecAction
    prog: |-
      from hcpdiff.utils.net_utils import to_cpu, to_cuda
      to_cpu(memory.unet)
  - _target_: hcpdiff.workflow.DecodeAction
    vae: ${hcp.from_memory:vae}
    offload: true
  - _target_: hcpdiff.workflow.SaveImageAction
    save_root: ${output_dir}
    image_type: png
