_base_:
  - cfgs/train/dataset/regularization_dataset.yaml
  - cfgs/train/train_base.yaml
  - cfgs/train/tuning_base.yaml

unet:
  -
    lr: 1e-6
    layers: # k,v of cross attention
      - 're:.*attn2\.to_k$'
      - 're:.*attn2\.to_v$'

## lora version of CustomDiffusion
#lora_unet:
#  -
#    lr: 1e-4
#    layers:
#      - 're:.*attn2\.to_k$'
#      - 're:.*attn2\.to_v$'

tokenizer_pt:
  train: # prompt tuning embeddings, needs to be created in advance
    - { name: 'pt-dog1', lr: 0.003 }

train:
  gradient_accumulation_steps: 1
  save_step: 100

  scheduler:
    name: 'constant_with_warmup'
    num_warmup_steps: 50
    num_training_steps: 600

model:
  pretrained_model_name_or_path: 'runwayml/stable-diffusion-v1-5'
  tokenizer_repeats: 1
  ema_unet: 0
  ema_text_encoder: 0

data:
  dataset1:
    batch_size: 4
    cache_latents: True

    source:
      data_source1:
        img_root: 'imgs/'
        prompt_template: 'prompt_tuning_template/object.txt'
        caption_file: null # path to image captions (file_words)

        word_names:
          pt1: sks
          class: dog
    bucket:
      _target_: hcpdiff.data.bucket.RatioBucket.from_files # aspect ratio bucket
      target_area: ${hcp.eval:"512*512"}
      num_bucket: 1

  dataset_class:
    batch_size: 1
    cache_latents: True
    loss_weight: 1.0

    source:
      data_source1:
        img_root: 'imgs/db_class'
        prompt_template: 'prompt_tuning_template/object.txt'
        caption_file: null

        word_names:
          class: dog
    bucket:
      _target_: hcpdiff.data.bucket.FixedBucket
      target_size: [512, 512]